{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beLospdZjvOP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings         # чтобы никто не мешал бесчинствам с кодом\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0-bFQI_RGCX"
   },
   "source": [
    "## Dataset and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "RJPmimiFkJne",
    "outputId": "5aa99939-ecd2-44f7-d309-dcb125e90cf1"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                        download=True, \n",
    "                                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                        download=True, \n",
    "                                        transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xavch3tSkqwj"
   },
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tX1zCC6pkFqA"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "sCwL4w0PkKTP",
    "outputId": "167779da-538e-47ea-eda5-8e5b2b43d9cf"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "print(images[0].shape)\n",
    "print(labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "print(images[0].shape)\n",
    "print(labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgZVAOT0kNjN"
   },
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    \n",
    "    plt.figure(figsize=(batch_size * 4, 4))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdtOR7uUkcMV"
   },
   "outputs": [],
   "source": [
    "def show_batch_images(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    \n",
    "    img = torchvision.utils.make_grid(images)\n",
    "    imshow(img, title=[str(x.item()) for x in labels])\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "2LCi0WLikeFb",
    "outputId": "ccc5130b-6d6d-4a20-bb9b-88e9ee247cd3"
   },
   "outputs": [],
   "source": [
    "images, labels = show_batch_images(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = show_batch_images(testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBMI0JAQRKWV"
   },
   "source": [
    "## Batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспоминаем смысл batchnorm. Мы усредняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugjkClw71suO"
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(MyNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 48),  # 28 x 28 = 784\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "airXDQT52PGt"
   },
   "outputs": [],
   "source": [
    "class MyNetBN(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(MyNetBN, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 48),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(48, 24),\n",
    "            nn.BatchNorm1d(24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "4YDM_ITyl2vm",
    "outputId": "e627ade8-3d73-4147-bcfb-c2c36106c89e"
   },
   "outputs": [],
   "source": [
    "model = MyNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "BXQjwMiIphyT",
    "outputId": "788dc2d5-f7d6-4346-8614-7b146b1586d4"
   },
   "outputs": [],
   "source": [
    "model_bn = MyNetBN()\n",
    "print(model_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4z-fCHd1nH3H"
   },
   "outputs": [],
   "source": [
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cASdqPBfnH3N"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpWwJJP5mXgx"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=0.01)\n",
    "opt_bn = optim.SGD(model_bn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6895
    },
    "id": "FANQphl-mAp1",
    "outputId": "96b617b6-08e8-4832-b4da-4fe2d9cc6a30"
   },
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "loss_bn_arr = []\n",
    "\n",
    "loss_arr_test = []\n",
    "loss_bn_arr_test = []\n",
    "\n",
    "max_epochs = 2\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        # training steps for normal model\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # training steps for bn model\n",
    "        opt_bn.zero_grad()\n",
    "        outputs_bn = model_bn(inputs)\n",
    "        loss_bn = loss_fn(outputs_bn, labels)\n",
    "        loss_bn.backward()\n",
    "        opt_bn.step()\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        loss_bn_arr.append(loss_bn.item())\n",
    "        \n",
    "        model.eval()\n",
    "        model_bn.eval()\n",
    "        inputs_test, labels_test = next(iter(testloader))\n",
    "        test_pred = model(inputs_test)\n",
    "        test_loss = loss_fn(test_pred, labels_test)\n",
    "        test_pred_bn = model_bn(inputs_test)\n",
    "        test_loss_bn = loss_fn(test_pred_bn, labels_test)\n",
    "        loss_arr_test.append(test_loss.item())\n",
    "        loss_bn_arr_test.append(test_loss_bn.item())\n",
    "        \n",
    "        model.train()\n",
    "        model_bn.train()\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "        \n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            model.eval()\n",
    "            model_bn.eval()\n",
    "            \n",
    "            a = model.classifier[0](inputs)\n",
    "            a = model.classifier[1](a)\n",
    "            a = model.classifier[2](a)\n",
    "            a = a.detach().numpy().ravel()\n",
    "            sns.distplot(a, kde=True, color='r', label='Normal') \n",
    "            \n",
    "            b = model_bn.classifier[0](inputs)\n",
    "            b = model_bn.classifier[1](b)\n",
    "            b = model_bn.classifier[2](b)\n",
    "            b = model_bn.classifier[3](b)\n",
    "            b = model_bn.classifier[4](b)\n",
    "            b = b.detach().numpy().ravel()\n",
    "            \n",
    "            sns.distplot(b, kde=True, color='g', label='BatchNorm') \n",
    "            plt.title('%d: Loss = %0.2f, Loss with bn = %0.2f' % (i, loss.item(), loss_bn.item()))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.pause(0.5)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            model.train()\n",
    "            model_bn.train()\n",
    "            \n",
    "        \n",
    "        \n",
    "print('----------------------')\n",
    "\n",
    "plt.plot(loss_arr, 'r', label='Normal')\n",
    "plt.plot(loss_bn_arr, 'g', label='BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------------')\n",
    "\n",
    "# plt.plot(loss_arr, 'r', label='Normal')\n",
    "# plt.plot(loss_bn_arr, 'g', label='BatchNorm')\n",
    "plt.plot(loss_arr_test, 'r--', label='Normal_test')\n",
    "plt.plot(loss_bn_arr_test, 'g--', label='BatchNorm_test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такс - а теперь давайте реализуем 3х слойную сеточку с Batchnorm в серединке сетки.\n",
    "Размерности следующие - 64,48,24 нейрона на каждом слое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetNEW(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(MyNetNEW, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = MyNetNEW()\n",
    "print(model_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = MyNetBN()\n",
    "print(model_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=0.01)\n",
    "opt_bn = optim.SGD(model_bn.parameters(), lr=0.01)\n",
    "opt_new = optim.SGD(model_new.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_arr = []\n",
    "loss_bn_arr = []\n",
    "loss_new_arr = []\n",
    "\n",
    "loss_arr_test = []\n",
    "loss_bn_arr_test = []\n",
    "loss_new_arr_test = []\n",
    "\n",
    "max_epochs = 2\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        inputs, labels = data\n",
    "\n",
    "        # training steps for normal model\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # training steps for bn model\n",
    "        opt_bn.zero_grad()\n",
    "        outputs_bn = model_bn(inputs)\n",
    "        loss_bn = loss_fn(outputs_bn, labels)\n",
    "        loss_bn.backward()\n",
    "        opt_bn.step()\n",
    "        \n",
    "        # training steps for hw model\n",
    "        opt_new.zero_grad()\n",
    "        outputs_new = model_new(inputs)\n",
    "        loss_new = loss_fn(outputs_new, labels)\n",
    "        loss_new.backward()\n",
    "        opt_new.step()\n",
    "        \n",
    "        loss_arr.append(loss.item())\n",
    "        loss_bn_arr.append(loss_bn.item())\n",
    "        loss_new_arr.append(loss_new.item())\n",
    "        \n",
    "        model.eval()\n",
    "        model_bn.eval()\n",
    "        model_new.eval()\n",
    "        inputs_test, labels_test = next(iter(testloader))\n",
    "        test_pred = model(inputs_test)\n",
    "        test_loss = loss_fn(test_pred, labels_test)\n",
    "        test_pred_bn = model_bn(inputs_test)\n",
    "        test_loss_bn = loss_fn(test_pred_bn, labels_test)\n",
    "        test_pred_new = model_new(inputs_test)\n",
    "        test_loss_new = loss_fn(test_pred_new, labels_test)\n",
    "        loss_arr_test.append(test_loss.item())\n",
    "        loss_bn_arr_test.append(test_loss_bn.item())\n",
    "        loss_new_arr_test.append(test_loss_new.item())\n",
    "        \n",
    "        model.train()\n",
    "        model_bn.train()\n",
    "        model_new.train()\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "        \n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            \n",
    "            model.eval()\n",
    "            model_bn.eval()\n",
    "            model_new.eval()\n",
    "            \n",
    "            a = model.classifier[0](inputs)\n",
    "            a = model.classifier[1](a)\n",
    "            a = model.classifier[2](a)\n",
    "            a = a.detach().numpy().ravel()\n",
    "            sns.distplot(a, kde=True, color='r', label='Normal') \n",
    "            \n",
    "            b = model_bn.classifier[0](inputs)\n",
    "            b = model_bn.classifier[1](b)\n",
    "            b = model_bn.classifier[2](b)\n",
    "            b = model_bn.classifier[3](b)\n",
    "            b = model_bn.classifier[4](b)\n",
    "            b = b.detach().numpy().ravel()\n",
    "            sns.distplot(b, kde=True, color='g', label='BatchNorm') \n",
    "    \n",
    "            c = model_new.classifier[0](inputs)\n",
    "            c = model_new.classifier[1](c)\n",
    "            c = model_new.classifier[2](c)\n",
    "            c = model_new.classifier[3](c)\n",
    "#             c = model_hw.classifier[4](c)\n",
    "            c = c.detach().numpy().ravel()\n",
    "            \n",
    "            sns.distplot(c, kde=True, color='b', label='NEW BatchNorm') \n",
    "            plt.title('%d: Loss = %0.2f, Loss with bn = %0.2f, Loss new = %0.2f' % (i, loss.item(), loss_bn.item(), loss_new.item()))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            plt.pause(0.5)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            model.train()\n",
    "            model_bn.train()\n",
    "            model_new.train()\n",
    "            \n",
    "        \n",
    "        \n",
    "print('----------------------')\n",
    "\n",
    "plt.plot(loss_arr, 'r', label='Normal')\n",
    "plt.plot(loss_bn_arr, 'g', label='BatchNorm')\n",
    "plt.plot(loss_new_arr, 'b', label='NEW BatchNorm')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7Wbb3yyRZ1m"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный прием отключает наш нейрон в сеточке при обучении с какой-то вероятностью. Давайте посмотрим на практике, как это работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jE_uh4MRdUU"
   },
   "outputs": [],
   "source": [
    "N = 50\n",
    "noise = 0.5\n",
    "\n",
    "X_train = torch.unsqueeze(torch.linspace(-1, 1, N), 1)\n",
    "Y_train = X_train + noise * torch.normal(torch.zeros(N, 1), torch.ones(N, 1))\n",
    "\n",
    "X_test = torch.unsqueeze(torch.linspace(-1, 1, N), 1)\n",
    "Y_test = X_test + noise * torch.normal(torch.zeros(N, 1), torch.ones(N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "ddZ9F9umR-9X",
    "outputId": "a53ba1c7-65a0-4648-8138-4255e28096d3"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
    "plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOOiuwL9Sf_4"
   },
   "outputs": [],
   "source": [
    "\n",
    "N_1 = 100\n",
    "N_2 = 60 \n",
    "p = 0.2\n",
    "# dropout добавляется следующей командой torch.nn.Dropout()\n",
    "# модель можно создать просто model_without_drop = torch.nn.Sequential()\n",
    "# Нашим заданием будет создать 2 модельки model_without_drop и model_dropout. с N_1 и N_2 количеством нейронов\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1, N_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_1, N_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_2, 1)\n",
    "        )\n",
    "\n",
    "model_dropout = nn.Sequential(\n",
    "            nn.Linear(1, N_1),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_1, N_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_2, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QrGLYVfTdmk"
   },
   "outputs": [],
   "source": [
    "# Оптимизаторы возьмем Adam, а минимизировать будем MSE\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "opt_dropout = optim.Adam(model_dropout.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5297
    },
    "id": "CQNTiF2MT2Tw",
    "outputId": "b346dcf7-5fcc-4c6c-9adb-1e24ad1f8c89",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 1000\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    pred = model(X_train)\n",
    "    loss = loss_fn(pred, Y_train)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    pred_dropout = model_dropout(X_train)\n",
    "    loss_dropout = loss_fn(pred_dropout, Y_train)\n",
    "    opt_dropout.zero_grad()\n",
    "    loss_dropout.backward()\n",
    "    opt_dropout.step()\n",
    "    \n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        model_dropout.eval()\n",
    "        \n",
    "        test_pred = model(X_test)\n",
    "        test_loss = loss_fn(test_pred, Y_test)\n",
    "        \n",
    "        test_pred_dropout = model_dropout(X_test)\n",
    "        test_loss_dropout = loss_fn(test_pred_dropout, Y_test)\n",
    "        \n",
    "        plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n",
    "        plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n",
    "        plt.plot(X_test.data.numpy(), test_pred.data.numpy(), 'r-', lw=3, label='normal')\n",
    "        plt.plot(X_test.data.numpy(), test_pred_dropout.data.numpy(), 'b--', lw=3,  label='dropout')\n",
    "        \n",
    "        plt.title('Epoch %d, Loss = %0.4f, Loss with dropout = %0.4f' % (epoch, test_loss, test_loss_dropout))\n",
    "        \n",
    "        plt.legend()\n",
    "\n",
    "        model.train()\n",
    "        model_dropout.train()\n",
    "        \n",
    "        plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошая инициализация весов нам нужна, чтобы наш сигнал не терялся и не разлетался в глубоких сетках. \n",
    "Для этого мы пытаемся сохранить постоянную дисперсию между слоями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Добавим еще обучение по сценарию\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "\n",
    "class FeedforwardNeuralNetModel_with_Kaiming(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel_with_Kaiming, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Linear weight, W,  Y = WX + B\n",
    "        nn.init.normal_(self.fc1.weight, mean=0, std=1)\n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.normal_(self.fc2.weight, mean=0, std=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model_he = FeedforwardNeuralNetModel_with_Kaiming(input_dim, hidden_dim, output_dim)\n",
    "model_simple = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer_simple = torch.optim.SGD(model_simple.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "optimizer_he = torch.optim.SGD(model_he.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "\n",
    "# step_size: Как часто мы будем уменьшать наш LR\n",
    "# step_size = 1, после каждой эпохи new_lr = lr*gamma \n",
    "# step_size = 2, после каждой второй эпохи new_lr = lr*gamma \n",
    "\n",
    "# gamma = насколько сильно снижаем\n",
    "scheduler_simple = StepLR(optimizer_simple, step_size=1, gamma=0.96)\n",
    "scheduler_he = StepLR(optimizer_he, step_size=1, gamma=0.96)\n",
    "\n",
    "'''\n",
    "TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "loss_arr_simple = []\n",
    "loss_arr_he = []\n",
    "\n",
    "acc_arr_simple = []\n",
    "acc_arr_he = []\n",
    "iterations = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Decay Learning Rate\n",
    "    scheduler_simple.step()\n",
    "    scheduler_he.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', epoch,'LR:', scheduler_simple.get_lr())\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, 28*28)\n",
    "        \n",
    "        # training steps for normal model\n",
    "        optimizer_simple.zero_grad()\n",
    "        outputs_simple = model_simple(images)\n",
    "        loss_simple = loss_fn(outputs_simple, labels)\n",
    "        loss_simple.backward()\n",
    "        optimizer_simple.step()\n",
    "        \n",
    "       # train step for He ini\n",
    "        optimizer_he.zero_grad()\n",
    "        outputs_he = model_he(images)\n",
    "        loss_he = loss_fn(outputs_he, labels)\n",
    "        loss_he.backward()\n",
    "        optimizer_he.step()\n",
    "        \n",
    "        loss_arr_simple.append(loss_simple.item())\n",
    "        loss_arr_he.append(loss_he.item())\n",
    "\n",
    "        \n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "        if iterations % 100 == 0:\n",
    "            model_simple.eval()\n",
    "            model_he.eval()\n",
    "            # Calculate Accuracy  \n",
    "            total = 0\n",
    "            \n",
    "            correct_simple = 0\n",
    "            correct_he = 0\n",
    "\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in testloader:\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)  \n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, 28*28)\n",
    "                \n",
    "                # Forward pass only to get logits/output\n",
    "                outputs_simple = model_simple(images)\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted_simple = torch.max(outputs_simple.data, 1)\n",
    "                # Total correct predictions\n",
    "                correct_simple += (predicted_simple.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "                \n",
    "                # Повторяем для He\n",
    "                outputs_he = model_he(images)\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted_he = torch.max(outputs_he.data, 1)\n",
    "                # Total correct predictions\n",
    "                correct_he += (predicted_he.type(torch.FloatTensor).cpu() == labels.type(torch.FloatTensor)).sum()\n",
    "\n",
    "            accuracy_simple = 100. * correct_simple.item() / total\n",
    "            accuracy_he = 100. * correct_he.item() / total\n",
    "            acc_arr_simple.append(accuracy_simple)\n",
    "            acc_arr_he.append(accuracy_he)\n",
    "            model_simple.train()\n",
    "            model_he.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Accuracy for normal and he initialization')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(range(1, len(acc_arr_simple) + 1), acc_arr_simple, c='r', label='simple')\n",
    "plt.plot(range(1, len(acc_arr_he) + 1), acc_arr_he, c='b', label='he')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Источники данной тетрадки\n",
    "\n",
    "1) https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/weight_initialization_activation_functions/\n",
    "\n",
    "\n",
    "2) https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"0516_BatchNorm_Dropout.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
